{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dbb3eb0-f121-4f1d-831e-8ae0be666e05",
   "metadata": {},
   "source": [
    "# Using RAPIDS cuML \n",
    "**Rapids** - an open-source data analytics and machine learning acceleration platform created by **NVIDIA** <br>\n",
    "* leverages GPUs to accelerate computations<br>\n",
    "* based on Python, has pandas-like and Scikit-Learn-like interfaces<br>\n",
    "* built on Apache Arrow in-memory data format<br>\n",
    "* can scale from 1 to multi-GPU to multi-nodes<br>\n",
    "\n",
    "### ssh into Polaris login node\n",
    "1. ssh into one of Polaris' login nodes <br>\n",
    "`ssh username@polaris.alcf.anl.gov` <br>\n",
    "\n",
    "Clone the project repository into your home directory:<br>\n",
    "git clone https://github.com/argonne-lcf/ALCF_Hands_on_HPC_Workshop/ <br>\n",
    "cd /ALCF_Hands_on_HPC_Workshop/Scikit-learn<br>\n",
    "Copy files: `start_rapids_cluster_polaris.sh`, `start_rapids_cluster_rank.sh` and `activate_rapids_env_polaris.sh` to your $HOME dir and change their permisions (e.g. using `chmod 755 activate_rapids_env_polaris.sh`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dd76b4-25b2-4db9-ad89-3cb5e8298413",
   "metadata": {},
   "source": [
    "### Start a Dask-RAPIDS cluster on Polaris\n",
    "\n",
    "2. ##### <a href='https://docs.alcf.anl.gov/polaris/running-jobs/' target=\"_blank\">Submit an interactive job</a> on n (here 1) nodes \n",
    "    `qsub -l select=1:system=polaris -l filesystems=home:eagle:grand -l walltime=00:30:00 -I -A YourProject -q YourQueue` \n",
    "A shell opens up on one of the compute nodes <br>\n",
    "4. Run the script, `start_rapids_clusters_polaris.sh`. This will start the scheduler on the node with one worker per GPU.<br> \n",
    "If the cluster does not start, check the file `~/dask-local-directory/scheduler.json` and the log files of scheduler and workers in `~/dask-local-directory/logs/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc63bbc0-2019-43f3-9f56-87bbbd5c6bf7",
   "metadata": {},
   "source": [
    "### Access the cluster from JupyterLab\n",
    "1. Start a RAPIDS cluster\n",
    "2. Establish a <a href=\"https://en.wikibooks.org/wiki/OpenSSH/Cookbook/Multiplexing\" target='plain'>Multiplexed SSH Connection</a> to Polaris. Run the code below in your local machine, ensure to edit `YourUsername`<br>\n",
    "`ssh -M -S ~/.ssh/multiplex:polaris.rapids YourUsername@polaris.alcf.anl.gov`\n",
    "3. On a different terminal window in your local machine, run the script `open_jupyterlab_polaris.sh` to start a JupyterLab session on Polaris<br>\n",
    "`./open_jupyterlab_polaris.sh <COMPUTE_NODE_ADDRESS>` <br>\n",
    "where `<COMPUTE_NODE_ADDRESS>` is the compute node hostname, like `x3006c0s1b1n0`<br>\n",
    "4. Copy the url that is returned by the script and paste it in a browser window to access JupyterLab and view the dashboards <br>\n",
    "5. If it issued an ssh connection with control master error, open the script and replace `polaris` with <br>\n",
    "`YourUsername@polaris.alcf.anl.gov`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c025f631-2ed8-4aab-9dde-56fadf2d559d",
   "metadata": {},
   "source": [
    "### On JupyterLab\n",
    "Select the kernel with your RAPIDS' enviroment name (something like Python `[conda env:rapids-23.04_polaris]`) from the Kernel menu in the top right corner.<br>\n",
    "if the RAPIDS' kernel is not present in the kernel menu, add it by activating the conda environment running the commands below on Polaris login node <br>\n",
    "`2024-04-29//lus/grand/projects/alcf_training/rapids/polaris/rapids-23.04_polaris`<br>\n",
    "`python -m ipykernel install --user --name \"rapids-23.04\" --display-name \"Python [conda env:\"rapids-23.04\"]\"`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe90d1-9e03-4d18-b41f-61150dc0c98e",
   "metadata": {},
   "source": [
    "### Accessing Project Folders\n",
    "To access project directories located outside of your `$HOME`, a symbolic link to the directory must be created.<br>\n",
    "**from terminal**<br>\n",
    "cd ~\n",
    "`ln -s /ALCF_Hands_on_HPC_Workshop/Scikit-learn`\n",
    "\n",
    "**in notebook using `!` escape** <br>\n",
    "`!ln -s /ALCF_Hands_on_HPC_Workshop/Scikit-learn`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb70a59-fc56-4d69-a745-8e8fe9ac2186",
   "metadata": {},
   "source": [
    "### On Jupyter Notebook, Connect to the cluster with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3181e86-d086-4976-9adf-04c16dfa20ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import dask.distributed\n",
    "import json\n",
    "import pathlib\n",
    "fname = f'{pathlib.Path.home().as_posix()}/dask-local-directory/scheduler.json'\n",
    "with open(fname, 'r') as f:\n",
    "    scheduler = json.load(f)\n",
    "    client = dask.distributed.Client(scheduler['address'])\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2d13e6-ee6a-4806-88ab-cdf69b5d8ff1",
   "metadata": {},
   "source": [
    "### GPU Accelerated DBSCAN in RAPIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0bca30-450c-4a1d-be85-88df1ce2c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the neccessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "import cudf\n",
    "from cuml.dask.cluster import DBSCAN as daskDBSCAN\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cbffa1-6b5f-4d98-8f26-cd83dacf7b2a",
   "metadata": {},
   "source": [
    "**Generate Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c24c63-019b-4f53-867c-4307ae93da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data using make_blobs\n",
    "centers = [[0, 0], [0, 1]]\n",
    "X, _ = make_blobs(n_samples=180_000, centers=centers, cluster_std=0.09, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654fd0f7-5cff-4812-81f4-500d7d74f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our dataset\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20844521-7edc-4d1f-8941-a16b4b2df623",
   "metadata": {},
   "source": [
    "#### Train DBSCAN on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9644ec6-b598-4fbd-9aab-a6485e288166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to cuDF DataFrame for GPU\n",
    "X_df=pd.DataFrame(X)\n",
    "X_gpu = cudf.DataFrame.from_pandas(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0e55a-870b-425a-b6e1-08a97eae27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DBSCAN on GPUs using cuML\n",
    "start_time = time.time()\n",
    "db_gpu = daskDBSCAN(eps=0.2, min_samples=10).fit(X_gpu)\n",
    "gpu_time = time.time() - start_time\n",
    "print(f\"DBSCAN on GPU took: {gpu_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9c7b4-71a7-4eee-a819-947f7305bf28",
   "metadata": {},
   "source": [
    "#### Train DBSCAN on CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca3caeb-9eec-4aa4-a9ef-00941e55635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DBSCAN on CPU using scikit-learn\n",
    "start_time = time.time()\n",
    "db_cpu = DBSCAN(eps=0.2, min_samples=20).fit(X)\n",
    "cpu_time = time.time() - start_time\n",
    "print(f\"DBSCAN on CPU took: {cpu_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad37a1-7ab7-4096-aeac-22b144a0e843",
   "metadata": {},
   "source": [
    "#### Compare results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df821e5b-7b3c-4737-b615-3f138acf7fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "cpu_labels = db_cpu.labels_\n",
    "gpu_labels = db_gpu.labels_.to_pandas().values\n",
    "silhouette_gpu = silhouette_score(X, gpu_labels)\n",
    "print(f'Silhouette Score (GPU): {silhouette_gpu:.4f}')\n",
    "\n",
    "silhouette_cpu = silhouette_score(X, cpu_labels)\n",
    "print(f'Silhouette Score (CPU): {silhouette_cpu:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae21d19-719e-456b-8cfd-e7e7320598ef",
   "metadata": {},
   "source": [
    "\n",
    "### Shutdown the cluster with: \n",
    "`client.shutdown()`<br>\n",
    "Then run the script `close_jupyterlab_polaris.sh` on your local machine to end the JupyterLab session and close the multiplexed connection<br>\n",
    "Make sure to edit `YourUsername` on the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aaf273-8df5-4a8a-9365-6f21f238d24e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rapids-24.08_polaris]",
   "language": "python",
   "name": "rapids-24.08_polaris"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
