{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "osBfzgmiKSqs"
   },
   "source": [
    "# Reinforcement Learning Tutorial\n",
    "presented by Taylor Childers (Argonne)\n",
    "\n",
    "Content and examples based on this blog post from:\n",
    "Adam Kleczewski (PhD. in Physics)\n",
    "https://adamklecz.wordpress.com/2017/10/30/tic-tac-tensorflow/\n",
    "\n",
    "In the tutorial, we build a Tic Tac Toe gaming environment with the games rules in which our players can make moves and learn the best methods for optimizing the result, i.e. winning the game.\n",
    "\n",
    "# Deep Q-learning\n",
    "https://en.wikipedia.org/wiki/Q-learning\n",
    "\n",
    "Reinforment Learning uses a Q-learning method, which aims to find a state-action value function\n",
    "\n",
    "`Q(s,a)` \n",
    "\n",
    "that can characterize the value of taking action `a` given state `s`. \n",
    "\n",
    "In Deep Q-Learning this function is represented with a neural network. It can be a simple dense neural network, as we will use here, or  more complicated using Convolutional NNs like those used in DeepMind.\n",
    "\n",
    "In the following examples we also use a simpler state-only value function `V(s)` which only takes the current state of the game and calculates the value of that state. \n",
    "\n",
    "# Learning to Play Tic Tac Toe\n",
    "\n",
    "We will use Tic Tac Toe as an example state-space to explore Reinforment Learning.\n",
    "\n",
    "We are looking to train a neural network to predict the value of making a move in our Tic Tac Toe board. \n",
    "\n",
    "Our model will be simple with one hidden layer. Our input will encode the current state of the board including who's turn it is. This is arranged as a flat input with the following binary features: location of Xs, location of Os, empty locations, and one value to represent whose turn it is.\n",
    "\n",
    "The model outputs a prediction for the maximum possible reward expected at the end of the game given the action inside the current board state. This model represents our `V(s)` and is typically called the 'Policy' because it encompasses the movement policy of our players.\n",
    "\n",
    "![model_graph](https://adamklecz.files.wordpress.com/2017/10/null9.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_hac3awMhBAe",
    "outputId": "9d793c2e-c631-4212-af81-4badf4660404",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load up necessary modules\n",
    "import numpy as np\n",
    "import logging,copy,os,sys\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipydis\n",
    "# turn off all the deprecation warnings\n",
    "tf.logging.set_verbosity(logging.ERROR)\n",
    "# turn on eager execution\n",
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager\n",
    "print('Tensorflow Version: %s',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pkAqbtdNvEOD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here is our model to be trained\n",
    "class ValueModel(tf.keras.Model):\n",
    "  def __init__(self, input_dim, hidden_dim = 100):\n",
    "    super(ValueModel, self).__init__()\n",
    "    # this is our dense hidden layer witha ReLU activiation that will encode most of our information\n",
    "    self.hidden_layer = tf.keras.layers.Dense(hidden_dim,'relu',input_shape=(input_dim,),use_bias=False)\n",
    "    # then we reduce to a single output with a tanh activation\n",
    "    # we use tanh because -1 <= tanh(x) <= 1 and we will build a reward system based on a range -1 to 1\n",
    "    self.output_layer = tf.keras.layers.Dense(1,'tanh',use_bias=False)\n",
    "\n",
    "  def call(self,input):\n",
    "    # this is the function used to actually evaluate our model on input data\n",
    "    x = self.hidden_layer(input)\n",
    "    x = self.output_layer(x)\n",
    "    return x\n",
    "\n",
    "# Just quickly look at our model \n",
    "test_model = ValueModel(28)\n",
    "# in eager execution, we must run the model once to trigger TF to build it\n",
    "test_model(np.random.rand(1,28))\n",
    "# print a nice summary from Keras\n",
    "test_model.summary()\n",
    "# print the trainable parameters\n",
    "print([x.shape for x in test_model.trainable_variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLrxrVGDg-TA"
   },
   "source": [
    "# Building a Tic Tac Toe Board\n",
    "We need to build our tic tac toe board. This will hold a representation of the board in a given state. \n",
    "\n",
    "We keep track of our board with two sub-boards; one for X and one for O. \n",
    "\n",
    "We also keep track of who's turn it is and a list of possible moves that remain. \n",
    "\n",
    "I've seen many different implementations of this representation so there is nothing special about this method. It's just important you have all the information you need to represent the state of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dIu3G8Qx2euo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TicTacToeBoard:\n",
    "  def __init__(self):\n",
    "    # setup two boards\n",
    "    # one to track location of X\n",
    "    self.xs = np.zeros((3, 3))\n",
    "    # one to track location of O\n",
    "    self.os = np.zeros((3, 3))\n",
    "    # keep list of legal moves, as moves are made, \n",
    "    # they are removed from this list\n",
    "    self.legal_moves = set(range(9))\n",
    "    # keep track of who's turn it is: True == X-turn, False == O-turn\n",
    "    self.turn = np.random.choice([True,False])\n",
    "  \n",
    "  def push(self, move):\n",
    "    # move as number from 0 - 8, calculate row/col\n",
    "    row = int(move / 3)\n",
    "    col = move % 3\n",
    "    # make sure spot is vacant\n",
    "    assert self.xs[row, col] == 0\n",
    "    assert self.os[row, col] == 0\n",
    "    \n",
    "    # make sure move is legal\n",
    "    assert move in self.legal_moves\n",
    "    # remove move from legal moves\n",
    "    self.legal_moves.remove(move)\n",
    "    \n",
    "    # place X or O based on turn\n",
    "    if self.turn:\n",
    "       self.xs[row, col] = 1\n",
    "    else:\n",
    "       self.os[row, col] = 1\n",
    "    # change turn\n",
    "    self.turn = not self.turn\n",
    "\n",
    "  def result(self):\n",
    "    # calculate if someone has won:\n",
    "    # X wins\n",
    "    if (self.xs.all(axis=0).any() or self.xs.all(axis=1).any() or \n",
    "        self.xs.diagonal().all() or np.rot90(self.xs).diagonal().all()):\n",
    "      return 1.0\n",
    "    # O wins\n",
    "    elif (self.os.all(axis=0).any() or self.os.all(axis=1).any() or \n",
    "          self.os.diagonal().all() or np.rot90(self.os).diagonal().all()):\n",
    "      return -1.0\n",
    "    # stale mate\n",
    "    elif (self.xs + self.os).sum() == 9:\n",
    "      return 0\n",
    "    # game not yet finished\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "  def copy(self):\n",
    "    return copy.deepcopy(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uTD0HlvYQ1I6"
   },
   "source": [
    "# Building the Game Environment\n",
    "Next we build an environment around the board. This environment will have a few helper methods to help manage the life-time of a game from start to finish. \n",
    "\n",
    "One helper method is the `make_feature_vector()` which creates the input vector to our machine learning model. This feature vector is simple a flat vector our our two boards, where no moves have been made, and who's turn it is.\n",
    "\n",
    "The `play()` method takes a list of two players (or agents), and runs a game from start to finish and returns the result (`1`=X-wins,  `-1`=O-wins,  `0`=stalemate). One full game is typically referred to as 'Policy Rollout' because we've followed our Policy to some end state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5O4TNIvKDy-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TicTacToeEnv:\n",
    "  def __init__(self):\n",
    "    # environment includes a board\n",
    "    self.board = TicTacToeBoard()\n",
    "    # environment includes a feature vector\n",
    "    # this is the input to our ML model\n",
    "    self.feature_vector_size = 28\n",
    "\n",
    "  def reset(self):\n",
    "    # can reset the board\n",
    "    self.board = TicTacToeBoard()\n",
    "\n",
    "  def get_reward(self):\n",
    "    # retrieve result of current board state\n",
    "    return self.board.result()\n",
    "\n",
    "  def make_move(self, move):\n",
    "    # make a move on the board\n",
    "    self.board.push(move)\n",
    "\n",
    "  def make_random_move(self):\n",
    "    # make a random move\n",
    "    legal_moves = self.get_legal_moves()\n",
    "    move = np.random.choice(legal_moves)\n",
    "    self.make_move(move)\n",
    "\n",
    "  def get_legal_moves(self):\n",
    "    # return a list of the current legal moves available\n",
    "    return list(self.board.legal_moves)\n",
    "\n",
    "  def make_feature_vector(self, board):\n",
    "    # build a feature vector from the current board\n",
    "    fv_size = self.feature_vector_size\n",
    "    # create empty feature vector\n",
    "    fv = np.zeros((1, fv_size))\n",
    "    # set X locations\n",
    "    fv[0, :9] = board.xs.reshape(9)\n",
    "    # set O locations\n",
    "    fv[0, 9:18] = board.os.reshape(9)\n",
    "    # set empty locations\n",
    "    fv[0, 18:27] = ((board.xs + board.os).reshape(9) == 0)\n",
    "    # set who's turn it is\n",
    "    fv[0, -1] = float(board.turn)\n",
    "    return fv\n",
    "\n",
    "  def play(self, players, verbose=False):\n",
    "    # players is a list of agents playing the game\n",
    "    \n",
    "    # calculate current reward\n",
    "    reward = self.get_reward()\n",
    "    \n",
    "    # reward is None when game has not finished\n",
    "    while reward is None:\n",
    "      if verbose:\n",
    "        self._print()\n",
    "      # this simply rotates between players\n",
    "      # if it was X's turn, it returns O's turn\n",
    "      player = players[int(not self.board.turn)]\n",
    "      # get a move from the agent\n",
    "      move = player.get_move()\n",
    "      # apply the move to the environment\n",
    "      self.make_move(move)\n",
    "      # recalculate the reward, while-loop repeats if not finished\n",
    "      reward = self.get_reward()\n",
    "\n",
    "    if verbose:\n",
    "      self._print()\n",
    "      if reward == 1:\n",
    "        print(\"X won!\")\n",
    "      elif reward == -1:\n",
    "        print(\"O won!\")\n",
    "      else:\n",
    "        print(\"draw.\")\n",
    "    return reward\n",
    "  \n",
    "  def _print(self, board=None):\n",
    "    # clear the disply\n",
    "    ipydis.clear_output(wait=True)\n",
    "    # print the current board\n",
    "    if board is None:\n",
    "      board = self.board\n",
    "    s = ''\n",
    "    for i in range(3):\n",
    "      s += ' '\n",
    "      for j in range(3):\n",
    "        if board.xs[i, j] == 1:\n",
    "          s += 'X'\n",
    "        elif board.os[i, j] == 1:\n",
    "          s += 'O'\n",
    "        else:\n",
    "          s += ' '\n",
    "        if j < 2:\n",
    "          s += '|'\n",
    "      s += '\\n'\n",
    "      if i < 2:\n",
    "        s += '-------\\n'\n",
    "    print(s)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVf9-0VrUnWu"
   },
   "source": [
    "# Building Game Players: Agents\n",
    "In reinforcement learning, agents are used to explore the environment and thereby train the model on best practices.\n",
    "\n",
    "We'll start with a base agent on which we will build our players.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhHtboGOm3tW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AgentBase:\n",
    "  def __init__(self, model, env):\n",
    "    # keep a local copy of the model and environment\n",
    "    self.model = model\n",
    "    self.env = env\n",
    "    \n",
    "    # an episode is one move\n",
    "    self.episode_count = 0\n",
    "    \n",
    "    # keep track of the number of wins/loss/draws\n",
    "    self.x_wins = 0\n",
    "    self.x_draws = 0\n",
    "    self.x_losses = 0\n",
    "\n",
    "    self.o_wins = 0\n",
    "    self.o_draws = 0\n",
    "    self.o_losses = 0\n",
    "    \n",
    "    # reset global step\n",
    "    tf.train.get_or_create_global_step().assign(0)\n",
    "    \n",
    "    # for learning rate decay\n",
    "    self.lr_init = 0.1\n",
    "    self.decay = tfe.Variable(0.75)\n",
    "    self.period = tfe.Variable(1.0e5)\n",
    "    self.lrvar = tfe.Variable(self.lr_init)\n",
    "  \n",
    "  # to use LR decay, call this function in your train function\n",
    "  def learning_rate_update(self):\n",
    "    lr = self.lr_init * tf.math.pow(self.decay,tf.math.floordiv(tf.cast(self.episode_count,tf.float32),self.period))\n",
    "    self.lrvar.assign(lr)\n",
    "  \n",
    "  # set learning rate decay variables\n",
    "  def set_lr_init(self,value):\n",
    "    self.lr_init = value\n",
    "    self.lrvar.assign(value)\n",
    "  def set_decay(self,value):\n",
    "    self.decay.assign(value)\n",
    "  def set_period(self,value):\n",
    "    self.period.assign(value)\n",
    "\n",
    "  def add_results(self,results):\n",
    "    self.x_wins    = results[0]\n",
    "    self.x_draws   = results[1]\n",
    "    self.x_losses  = results[2]\n",
    "    self.o_wins    = results[3]\n",
    "    self.o_draws   = results[4]\n",
    "    self.o_losses  = results[5]\n",
    "\n",
    "  def increment_episode(self):\n",
    "    self.episode_count += 1\n",
    "\n",
    "  def get_move(self):\n",
    "    # get a list of legal moves from the environment\n",
    "    legal_moves = self.env.get_legal_moves()\n",
    "    # compile a list of the resulting boards for all legal moves\n",
    "    candidate_boards = []\n",
    "    for move in legal_moves:\n",
    "      candidate_board = self.env.board.copy()\n",
    "      candidate_board.push(move)\n",
    "      candidate_boards.append(candidate_board)\n",
    "    \n",
    "    # stack these possible outcomes into a batch\n",
    "    feature_vectors = np.vstack(\n",
    "       [self.env.make_feature_vector(board) for board in candidate_boards])\n",
    "    \n",
    "    # get the predicted rewards for each outcome\n",
    "    values = self.model(feature_vectors)\n",
    "    values = values.numpy()\n",
    "    \n",
    "    # loop over boards\n",
    "    for idx, board in enumerate(candidate_boards):\n",
    "      # get the result, 1=X-wins, -1=O-wins, 0=draw, None=not done\n",
    "      result = board.result()\n",
    "      if result is not None:\n",
    "        # if there is a result, update the prediction for the board\n",
    "        values[idx] = result\n",
    "    \n",
    "    # if current player is X, look for board with the max because 1 = X-wins\n",
    "    if self.env.board.turn:\n",
    "      move_idx = np.argmax(values)\n",
    "    # if current player is O, look for board with the min because -1 = O-wins\n",
    "    else:\n",
    "      move_idx = np.argmin(values)\n",
    "    # choose the move for the board with the best result\n",
    "    move = legal_moves[move_idx]\n",
    "\n",
    "    return move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTfDDj4Op7l3"
   },
   "source": [
    "# SimpleAgent\n",
    "\n",
    "We build a SimpleAgent that trains the model in a manner similar to supervised learning methods.\n",
    "\n",
    "![simple_agent](https://adamklecz.files.wordpress.com/2017/10/null1.png)\n",
    "\n",
    "The agent starts with an empty board (`s_0`), and plays against itself, building up a list of intermediate game states (`s_i`), until the game ends (`s_T`) with the reward (`r_T`) being a win(`1`), loss(`-1`), or draw(`0`). During training, the model produces a predicted reward for each turn of the game, but the environment only provides a reward for the last move. Thus the loss for each `s_i` is calculated using the same final reward `r_T` for each board state in the game.\n",
    "\n",
    "This is the most simplistic agent and follows what is called a 'greedy policy.'\n",
    "\n",
    "Notice that the `train` function inserts a random move for `epsilon` fraction of the moves. This randomization ensures our model tries new strategies instead of only learning one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q5mqBrQYqqZj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SimpleAgent(AgentBase):\n",
    "  def __init__(self,model,env):\n",
    "    super(SimpleAgent,self).__init__(model,env)\n",
    "    \n",
    "    # using learning rate decay variable from base class\n",
    "    self.opt = tf.train.GradientDescentOptimizer(self.lrvar)\n",
    "\n",
    "  def loss(self,inputs,targets):\n",
    "    # get predicted rewards for set of inputs\n",
    "    results = self.model(inputs)\n",
    "    # calculate the difference between the prediction and actual result\n",
    "    loss = tf.reduce_mean(tf.abs(targets - results))\n",
    "    return loss\n",
    "\n",
    "  def grad(self,inputs,targets):\n",
    "    # get loss for inputs and targets\n",
    "    with tf.GradientTape() as tape: # <-- necesary for eager execution in TF\n",
    "      loss_value = self.loss(inputs, targets)\n",
    "    # return the gradients\n",
    "    return tape.gradient(loss_value, self.model.trainable_variables)\n",
    "\n",
    "  def train(self, epsilon):\n",
    "    # reset board\n",
    "    self.env.reset()\n",
    "    \n",
    "    # reward for current board state\n",
    "    reward = None\n",
    "    \n",
    "    # call learning rate update for each episode played\n",
    "    self.learning_rate_update()\n",
    "    \n",
    "    # list of board states for each turn, represented as input vectors\n",
    "    feature_vectors = []\n",
    "    \n",
    "    \n",
    "    # loop until reward is reached\n",
    "    while reward is None:\n",
    "      feature_vector = self.env.make_feature_vector(self.env.board)\n",
    "      feature_vectors.append(feature_vector)\n",
    "      \n",
    "      # insert some random fluctuations to ensure stategy space is fully explored\n",
    "      if np.random.random() < epsilon:\n",
    "        self.env.make_random_move()\n",
    "      # make the best move based on the current model\n",
    "      else:\n",
    "        move = self.get_move()\n",
    "        self.env.make_move(move)\n",
    "      \n",
    "      # get reward from current board (1=X-wins,-1=O-wins,0=draw,None=not-finished)\n",
    "      reward = self.env.get_reward()\n",
    "    \n",
    "    \n",
    "    # create an input batch out of the games moves\n",
    "    inputs = np.vstack(feature_vectors)\n",
    "    # calculate the gradients based on the reward for these inputs\n",
    "    grads = self.grad(inputs,reward)\n",
    "    # apply gradients to model\n",
    "    self.opt.apply_gradients(zip(grads, self.model.trainable_variables),\n",
    "                             global_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    # return reward\n",
    "    return self.env.get_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xcQhcSX6-9g"
   },
   "source": [
    "# Random Agent\n",
    "\n",
    "The training of the model is done with our SimpleAgent playing both sides, but we also need a way to validate that our agent is learning good strategy. For this simple case of Tic Tac Toe we can simply use a RandomAgent as our validation. \n",
    "\n",
    "This agent choses a random move each time. While this is not very sophisticated, it is sufficient in this simple game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctolrNuLyjrh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "  def __init__(self, env, games = 100):\n",
    "    self.env = env\n",
    "    self.games = games\n",
    "\n",
    "  def get_move(self):\n",
    "    # get a list of the legal moves remaining\n",
    "    legal_moves = self.env.get_legal_moves()\n",
    "    # chose randomly from this list\n",
    "    move = np.random.choice(legal_moves)\n",
    "    return move\n",
    "\n",
    "  def test(self, agent):\n",
    "    # play 100 games with RandomAgent as X and the passed agent as O\n",
    "    x_counter = Counter()\n",
    "    for _ in range(self.games):\n",
    "      agent.env.reset()\n",
    "      reward = agent.env.play([agent, self])\n",
    "      x_counter.update([reward])\n",
    "    \n",
    "    # now play 100 games with opposite players\n",
    "    o_counter = Counter()\n",
    "    for _ in range(self.games):\n",
    "      agent.env.reset()\n",
    "      reward = agent.env.play([self, agent])\n",
    "      o_counter.update([reward])\n",
    "\n",
    "    results = [x_counter[1], x_counter[0], x_counter[-1],\n",
    "               o_counter[-1], o_counter[0], o_counter[1]]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C3EyLm78zcOw"
   },
   "source": [
    "# Training our SimpleAgent\n",
    "Now we have all the tools to train our agent to play the Tic Tac Toe.\n",
    "\n",
    "Here is a training function that runs for some number of 'episodes' (or games) and tests the model periodically against the random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jkmlypzdz13T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_agent(agent_class, # agent class to train\n",
    "                episodes=100000, # number of episode to train\n",
    "                test_frequency=1000,  # frequency of testing against RandomAgent\n",
    "                model_hidden_layer_size=100, # number of features in hidden layer of model\n",
    "                random_move_rate=0.25, # how often to insert random move during training\n",
    "                plot=True, # plot data using pyplot\n",
    "                show_plot=True, # show plot in real time\n",
    "                restore=False, # restore the previous model at start up\n",
    "                model_fn=None, # provide a custom filename to load\n",
    "                lr_init=0.1,\n",
    "                lr_decay=0.75,\n",
    "                lr_period=1.0e6): \n",
    "  \n",
    "  # create our Environment which includes a board\n",
    "  env = TicTacToeEnv()\n",
    "  # create our Model, or our function Q(s,a)\n",
    "  model = ValueModel(env.feature_vector_size, model_hidden_layer_size)\n",
    "  # run model once to trigger TF to build the model\n",
    "  model(np.random.rand(1,env.feature_vector_size))\n",
    "  \n",
    "  # load model weights if a file exists\n",
    "  print(f'training agent: {agent_class.__name__}')\n",
    "  weight_fn = f'{agent_class.__name__}_agent.h5'\n",
    "  if restore and os.path.exists(weight_fn):\n",
    "    if model_fn is not None:\n",
    "      print(f'restoring file: {model_fn}')\n",
    "      # load the weights\n",
    "      model.load_weights(model_fn)\n",
    "    else:\n",
    "      print(f'restoring file: {weight_fn}')\n",
    "      # load the weights\n",
    "      model.load_weights(weight_fn)\n",
    "  \n",
    "  # create a simple and random agents\n",
    "  agent = agent_class(model, env)\n",
    "  # set learning rate parameters\n",
    "  agent.set_lr_init(lr_init)\n",
    "  agent.set_decay(lr_decay)\n",
    "  agent.set_period(lr_period)\n",
    "  # create the random agent for testing\n",
    "  random_agent = RandomAgent(env)\n",
    "  \n",
    "  \n",
    "  # ÀáÀáÀáÀáÀá this is for plotting only ÀáÀáÀáÀáÀá\n",
    "  if plot:\n",
    "    # keep a list of the win fraction of X and O\n",
    "    x_wf = []\n",
    "    o_wf = []\n",
    "    \n",
    "    # create a figure\n",
    "    fig = plt.figure(figsize=(8, 6), dpi=80)\n",
    "    # create a subplot in that faigure\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    image_fn = f'{agent_class.__name__}_agent.png'\n",
    "  # ^^^^^ this is for plotting only ^^^^^\n",
    "  \n",
    "  # loop for some number of game episodes\n",
    "  while agent.episode_count < episodes:\n",
    "    \n",
    "    # every now and then test the agent against the random agent\n",
    "    if agent.episode_count % test_frequency == 0:\n",
    "      # run test of agent vs random agent\n",
    "      results = random_agent.test(agent)\n",
    "      \n",
    "      # keep running sum of wins/losses/draws\n",
    "      agent.add_results(results)\n",
    "      \n",
    "      # calculate win fractions\n",
    "      x_win_fraction = results[0] / np.sum(results[0:3])\n",
    "      o_win_fraction = results[3] / np.sum(results[3:6])\n",
    "      \n",
    "      # save model weights to a file\n",
    "      model.save_weights(weight_fn)\n",
    "      \n",
    "      # ÀáÀáÀáÀáÀá this is for plotting only ÀáÀáÀáÀáÀá\n",
    "      if plot:\n",
    "        # keep history of win fractions\n",
    "        x_wf.append(x_win_fraction)\n",
    "        o_wf.append(o_win_fraction)\n",
    "        if show_plot:\n",
    "          # first clear the display of previous plot\n",
    "          ipydis.clear_output()\n",
    "        # set the x-axis limit range\n",
    "        ax.set_xlim(0,int(agent.episode_count/test_frequency)+1)\n",
    "        # clear all pre-existing data from axis\n",
    "        ax.cla()\n",
    "        # set x/y axis labels\n",
    "        ax.set_xlabel('episodes (x%s)'%test_frequency)\n",
    "        ax.set_ylabel('win fraction')\n",
    "        # create the x-axis data\n",
    "        xaxis = range(int(agent.episode_count/test_frequency)+1)\n",
    "        # here we actually create the plot of X\n",
    "        ax.plot(xaxis,x_wf,label=\"X\")\n",
    "        # plot Y\n",
    "        ax.plot(xaxis,o_wf,label=\"O\")\n",
    "        # add the legend\n",
    "        ax.legend()\n",
    "        ax.set_title(f'{agent_class.__name__} with {model_hidden_layer_size} hidden features')\n",
    "        if show_plot:\n",
    "          # display the plot\n",
    "          ipydis.display(fig)\n",
    "        fig.savefig(image_fn)\n",
    "      # ^^^^^ this is for plotting only ^^^^^\n",
    "      \n",
    "    # print the results\n",
    "      print(f'{agent.episode_count:05d}: results = ' + '%24s' % results + f' x_win_fraction = {x_win_fraction:5.3f}  o_win_fraction = {o_win_fraction:5.3f} lr = ' + str(agent.lrvar.numpy()))\n",
    "  \n",
    "    else:\n",
    "      # here we actually run the training of our agent.\n",
    "      reward = agent.train(random_move_rate)\n",
    "    \n",
    "    # increment episode number\n",
    "    agent.increment_episode()\n",
    "\n",
    "  # save model weights to a file\n",
    "  model.save_weights(weight_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our SimpleAgent class. I have had a lot of trouble getting this model to converge in a resonable time so I include an image here and a model file (`SimpleAgent_agent_100.h5`) that can be loaded to play. This agent tends to be difficult to train and may take a few executions to get a model that converges to above 90% accuracy.\n",
    "\n",
    "![SimpleAgentTraining](SimpleAgent_agent_100.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1135
    },
    "colab_type": "code",
    "id": "7LoB9M8FBHiI",
    "outputId": "7e10d44a-4ab7-4b03-fb4e-4b0567c267d6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_agent(SimpleAgent,1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0GnMUJnG7CE7"
   },
   "source": [
    "# Playing our trained agent\n",
    "\n",
    "Now lets play our trained agent and see how well the RandomAgent was at characterizing what our Agent had learned.\n",
    "\n",
    "First we build a HumanAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufsiceaL7zuV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class HumanAgent(object):\n",
    "  def __init__(self, env):\n",
    "    self.env = env\n",
    "\n",
    "  def get_move(self):\n",
    "    while True:\n",
    "      # get the legal moves\n",
    "      legal_moves = self.env.get_legal_moves()\n",
    "      # easier to play with values from 1 - 9\n",
    "      print_moves = [ i + 1 for i in legal_moves]\n",
    "      # ask human to make a play\n",
    "      print(f'Pick from these legal moves = {print_moves}')\n",
    "      sys.stdout.flush()\n",
    "      move = input(':>')\n",
    "      try:\n",
    "        # legal_moves are listed as 1-9 so subtract 1\n",
    "        move = int(move) - 1\n",
    "        # if valid move\n",
    "        if move in legal_moves:\n",
    "          # return move\n",
    "          return move\n",
    "      except ValueError:\n",
    "        print(\"Illegal move\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pwMVpg58ClZ"
   },
   "source": [
    "Now we build a function for playing our trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fjwScm_66-2x",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def play_agent(agent_class,\n",
    "               model_hidden_layer_size=100, # make sure this is the same as when it was trained\n",
    "               filename=None): # filename of model to load, None will use class name\n",
    "  # create our Environment which includes a board\n",
    "  env = TicTacToeEnv()\n",
    "  # create our Model, or our function Q(s,a)\n",
    "  model = ValueModel(env.feature_vector_size, model_hidden_layer_size)\n",
    "  \n",
    "  # load model weights if a file exists\n",
    "  if filename is None:\n",
    "    weight_fn = f'{agent_class.__name__}_agent.h5'\n",
    "  else:\n",
    "    weight_fn = filename\n",
    "  print(f'training agent: {weight_fn}')\n",
    "  if os.path.exists(weight_fn):\n",
    "    print(f'restoring file: {weight_fn}')\n",
    "    # have to run model once to build the weights\n",
    "    model(np.random.rand(1,env.feature_vector_size))\n",
    "    # load the weights\n",
    "    model.load_weights(weight_fn)\n",
    "  else:\n",
    "    print(f'no trained model exists for {agent_class.__name__}, run train_agent({agent_class.__name__}) first')\n",
    "  \n",
    "  # create a simple and random agents\n",
    "  agent = agent_class(model, env)\n",
    "  human = HumanAgent(env)\n",
    "  \n",
    "  # start play\n",
    "  env.play([human,agent],verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SBwryNRJ8ae9"
   },
   "source": [
    "now you can play previously trained models! I've included `SimpleAgent_agent_good.h5` as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5xJRd7N28Zvn",
    "outputId": "193bf0db-dd71-4269-e31e-5a5b49f1dc02",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "play_agent(SimpleAgent) # filename='SimpleAgent_agent_100.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WmYDv8TY3yLO"
   },
   "source": [
    "## Temporal difference learning\n",
    "\n",
    "![td_agent](https://adamklecz.files.wordpress.com/2017/10/null2.png)\n",
    "\n",
    "In the SImpleAgent, we trained on a full game with each step being labeled with the final result. However, ideally you would attribute some value to the each turn being taken. This is referred to as temporal difference learning which is another style of Q-Learning.\n",
    "\n",
    "We will update our gradients using the following:\n",
    "\n",
    "![tdagent_grads](https://s0.wp.com/latex.php?latex=%5Ctheta_%7Bt%2B1%7D+%3D+%5Ctheta_t+%2B+%5Cnabla+V%28s_t%29+%5Cdelta_t&bg=ffffff&fg=000000&s=0&zoom=2)\n",
    "\n",
    "In the TDAgent, you can see that for each move in the game we calculate the difference between the current reward and the previous reward, `ùõø`. These are then multiplied by our model's gradients, `ŒîV`, and added to the model's weights, `ùúΩ`, at each move.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPh96m4kR7Pn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TDAgent(AgentBase):\n",
    "  def __init__(self,model,env):\n",
    "    super(TDAgent,self).__init__(model,env)\n",
    "    \n",
    "    self.opt = tf.train.GradientDescentOptimizer(self.lrvar)\n",
    "    \n",
    "  def grad(self,inputs):\n",
    "    # start gradient tape\n",
    "    with tf.GradientTape() as tape:\n",
    "      # calculate reward for inputs using model\n",
    "      reward = self.model(inputs)\n",
    "    # return the reward and the gradients based on this reward\n",
    "    return reward,tape.gradient(reward, self.model.trainable_variables)\n",
    "\n",
    "  def train(self, epsilon):\n",
    "    # reset the board\n",
    "    self.env.reset()\n",
    "    \n",
    "    # call learning rate update for each episode played\n",
    "    self.learning_rate_update()\n",
    "    \n",
    "    # get the initial feature vector for empty board\n",
    "    feature_vector = self.env.make_feature_vector(self.env.board)\n",
    "    # get reward and model gradients for empty board\n",
    "    previous_value,previous_grads = self.grad(feature_vector)\n",
    "    \n",
    "    # reward for empty board (= None)\n",
    "    reward = self.env.get_reward()\n",
    "    \n",
    "    while reward is None:\n",
    "      \n",
    "      # make the first move, with some chance of a random move\n",
    "      if np.random.random() < epsilon:\n",
    "        self.env.make_random_move()\n",
    "      else:\n",
    "        move = self.get_move()\n",
    "        self.env.make_move(move)\n",
    "      \n",
    "      # get the reward for new board configuration\n",
    "      reward = self.env.get_reward()\n",
    "      \n",
    "      # get the new board representation\n",
    "      feature_vector = self.env.make_feature_vector(self.env.board)\n",
    "      \n",
    "      # calculate the new reward and gradients\n",
    "      if reward is None:\n",
    "        value,grads = self.grad(feature_vector)\n",
    "      else:\n",
    "        value = reward\n",
    "        _,grads = self.grad(feature_vector)\n",
    "      \n",
    "      # calculate the difference between this reward and the previous\n",
    "      delta = value - previous_value\n",
    "      # techincal detail\n",
    "      delta = tf.reshape(delta,(1,))\n",
    "      # need to weight all gradients by the delta\n",
    "      new_grads = [-delta * g for g in previous_grads]\n",
    "      \n",
    "      # now apply the gradients\n",
    "      self.opt.apply_gradients(zip(new_grads, self.model.trainable_variables),\n",
    "                               global_step=tf.train.get_or_create_global_step())\n",
    "      \n",
    "      # current measurement becomes the previous before moving on\n",
    "      previous_grads = grads\n",
    "      previous_value = value\n",
    "    \n",
    "    # when game is over, return who won\n",
    "    return self.env.get_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train. The TDAgent typically reaches > 90% accuracy after 20k episodes:\n",
    "![TDAgent_agent_100.png](TDAgent_agent_100.png)\n",
    "\n",
    "There is also a pre-trained file available for testing: `TDAgent_agent_100.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1438
    },
    "colab_type": "code",
    "id": "9TvN5qx96Dga",
    "outputId": "73dc161b-69bf-4369-dff3-3bb107871308",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_agent(TDAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets play our agent\n",
    "play_agent(TDAgent) # filname='TDAgent_agent_100.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3PrJb2_EUynm"
   },
   "source": [
    "# TD  Learning with Forward Looking Rewards\n",
    "\n",
    "The TDAgent only looks at the current state/reward in relation to the previous state/reward. We'd like to be a bit more sophisticated in that our model learns to consider the future possible rewards given the current state.\n",
    "\n",
    "![forward_agent](https://adamklecz.files.wordpress.com/2017/10/null3.png)\n",
    "\n",
    "If you run the TDAgent a few times you'll find it does not learn very fast and sometimes never converges to winning at the 90% level. You will also notice that it has a longer run time. This is because it evalulates the model, calculates gradients, and applies gradients for every move in an episode. The SimpleAgent processed entire episdes in one calculation.\n",
    "\n",
    "This ForwardAgent returns to only processing the gradients once per episode, however, it uses the progression of the game to calculate rewards for each move that are based on the moves that followed. The weights of our model (trainable_variables) are updated using this formula: \n",
    "\n",
    "![formula](https://s0.wp.com/latex.php?latex=%5Ctheta_%7Bt%2B1%7D+%3D%5Ctheta_t+%2B+%5Calpha+%5Cnabla+V%28s_t%29%C2%A0%5Csum%5Climits_%7Bj%3Dt%7D%5E%7BT-1%7D+%5Clambda%5E%7Bj-t%7D+%5Cdelta_t&bg=ffffff&fg=000000&s=0&zoom=1.5)\n",
    "\n",
    "where `ùõø` is the temporal difference in reward between moves, `ŒîV(s)` are the gradients, `ùõº` is a learning rate, and `ùúÜ` characterizes how much influence rewards of future moves should have on the current state.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TySbh7XBcf8X",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ForwardAgent(AgentBase):\n",
    "  def __init__(self,\n",
    "               model,\n",
    "               env,\n",
    "               lamda = 0.7):\n",
    "    super(ForwardAgent,self).__init__(model, env)\n",
    "    # use optimizer to apply gradients\n",
    "    self.opt = tf.train.AdamOptimizer()\n",
    "    # lambda limits the effects of future rewards: 1.0 == no limit, 0.0 == no future considered\n",
    "    self.lamda = lamda\n",
    "  \n",
    "  def grad(self,inputs):\n",
    "    # start gradient tape\n",
    "    with tf.GradientTape() as tape:\n",
    "      # calculate reward for inputs using model\n",
    "      reward = self.model(inputs)\n",
    "    # return the reward and the gradients based on this reward\n",
    "    return reward,tape.gradient(reward, self.model.trainable_variables)\n",
    "\n",
    "  def train(self, epsilon):\n",
    "    \n",
    "    # reset for new episode\n",
    "    self.env.reset()\n",
    "  \n",
    "    # going to keep a list of the values and gradients at each move\n",
    "    grads_seq = []\n",
    "    value_seq = []\n",
    "    # initial reward with empty board == None\n",
    "    reward = self.env.get_reward()\n",
    "\n",
    "    # run the game\n",
    "    while reward is None:\n",
    "      # get the input for our model\n",
    "      feature_vector = self.env.make_feature_vector(self.env.board)\n",
    "      # evaluate the model on the input to get the reward and gradients\n",
    "      value, grads = self.grad(feature_vector)\n",
    "      # append to list\n",
    "      value_seq.append(value)\n",
    "      grads_seq.append(grads)\n",
    "      \n",
    "      # make a move, sometimes a random one\n",
    "      if np.random.random() < epsilon:\n",
    "          self.env.make_random_move()\n",
    "      else:\n",
    "          move = self.get_move()\n",
    "          self.env.make_move(move)\n",
    "      \n",
    "      # get updated reward\n",
    "      reward = self.env.get_reward()\n",
    "    \n",
    "    # game is finished, append final reward (1==win,0=draw,-1=loss)\n",
    "    value_seq.append(np.array([reward]))\n",
    "    \n",
    "    # calculate the difference in the rewards at each step\n",
    "    delta_seq = np.array([j - i for i, j in zip(value_seq[:-1], value_seq[1:])])\n",
    "    \n",
    "    # loop over the gradients at each move\n",
    "    for t, grads in enumerate(grads_seq):\n",
    "      # sum the delta for all future steps, weighted by the lamda raised to the number of moves into the future\n",
    "      delta_sum = np.sum([(self.lamda ** j) * delta for j, delta in enumerate(delta_seq[t:])])\n",
    "      # update gradients for this step by the sum of the delta\n",
    "      new_grads = [-delta_sum * g for g in grads]\n",
    "      # apply these gradients\n",
    "      self.opt.apply_gradients(zip(new_grads, self.model.trainable_variables),\n",
    "                             global_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    # return the final reward\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ForwardAgent trains quickly, only 10-20k episodes needed. \n",
    "\n",
    "![ForwardAgent](ForwardAgent_agent_100.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2148
    },
    "colab_type": "code",
    "id": "btjGlmr5a9lU",
    "outputId": "0411e5c0-1060-456a-f6ad-16eec337f7df",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_agent(ForwardAgent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the agent. I've included `ForwardAgent_agent_100.h5` as a pretrained test agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1173
    },
    "colab_type": "code",
    "id": "5p-6-sd_bCcy",
    "outputId": "837b9e78-9a08-429c-f2d5-605052919b40",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "play_agent(ForwardAgent) # filename='ForwardAgent_agent_100.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD Learning with Backward Looking Rewards\n",
    "\n",
    "![backward_agent](https://adamklecz.files.wordpress.com/2017/10/null4.png)\n",
    "\n",
    "The nice feature of the `ForwardAgent` was that you could calculate all the game moves at once in your model, taking advantange of vectorized calculations to speed things up. However, some games are too complicated to calculate in their intirety and keep in memory. This is where the `BackwardAgent` can be helpful.\n",
    "\n",
    "This agent only considers previous moves in modifying its model. Again, `ùúÜ` is used to represent the weight applied to previous moves.\n",
    "\n",
    "To implement this we simply keep a copy of our trainable variables and at each step we decay the previous value of this copy by our `ùúÜ` value and then add the current weights. This encapsulates the 'historical' rewards. This history is referred to as an 'eligibility trace' in RL.\n",
    "\n",
    "You should recognize some of the pieces from the previous agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BackwardAgent(AgentBase):\n",
    "  def __init__(self,model,env,lamda = 0.7):\n",
    "    super(BackwardAgent,self).__init__(model, env)\n",
    "    self.opt = tf.train.AdamOptimizer()\n",
    "    self.lamda = lamda\n",
    "\n",
    "  def grad(self,inputs):\n",
    "    # start gradient tape\n",
    "    with tf.GradientTape() as tape:\n",
    "      # calculate reward for inputs using model\n",
    "      reward = self.model(inputs)\n",
    "    # return the reward and the gradients based on this reward\n",
    "    return reward,tape.gradient(reward, self.model.trainable_variables)\n",
    "\n",
    "  def train(self, epsilon):\n",
    "    # reset environment\n",
    "    self.env.reset()\n",
    "    \n",
    "    # Our eligibility trace, filled with the current weights of our model\n",
    "    traces = [np.zeros(tvar.shape)\n",
    "              for tvar in self.model.trainable_variables]\n",
    "    \n",
    "    #print('traces = %s' % traces[0])\n",
    "    \n",
    "    # current board state (empty)\n",
    "    feature_vector = self.env.make_feature_vector(self.env.board)\n",
    "    \n",
    "    # current reward for board state and model gradients\n",
    "    previous_value, previous_grads = self.grad(feature_vector)\n",
    "    # get first reward from the environment (None)\n",
    "    \n",
    "    reward = self.env.get_reward()\n",
    "    \n",
    "    # run episode until a player wins\n",
    "    while reward is None:\n",
    "      # make a move, sometimes make a random move\n",
    "      if np.random.random() < epsilon:\n",
    "        self.env.make_random_move()\n",
    "      else:\n",
    "        move = self.get_move()\n",
    "        self.env.make_move(move)\n",
    "      \n",
    "      # get the reward for new board state\n",
    "      reward = self.env.get_reward()\n",
    "      # get board feature vector\n",
    "      feature_vector = self.env.make_feature_vector(self.env.board)\n",
    "      \n",
    "      # evaluate value function on new board\n",
    "      if reward is None:\n",
    "        value, grads = self.grad(feature_vector)\n",
    "      else:\n",
    "        value = reward\n",
    "        _,grads = self.grad(feature_vector)\n",
    "      \n",
    "      #print(f'value = {value}')\n",
    "      #print('grads = %s' % grads[0])\n",
    "      #print('traces = %s' % traces[0])\n",
    "      \n",
    "      # calculate change in reward\n",
    "      delta = value - previous_value\n",
    "      # scale previous trace by lamda and add new gradient\n",
    "      for previous_grad, trace in zip(previous_grads, traces):\n",
    "        #print('trace = %s previous_grad = %s' % (trace[0],previous_grad[0]))\n",
    "        trace *= self.lamda\n",
    "        trace += previous_grad.numpy() # convert TF tensor to numpy to calculate\n",
    "        #print('trace = %s previous_grad = %s' % (trace[0],previous_grad[0]))\n",
    "      \n",
    "      #print('traces = %s' % traces[0])\n",
    "      \n",
    "      # update gradients for this step by the sum of the delta\n",
    "      new_grads = [np.reshape(-delta * t,t.shape) for t in traces]\n",
    "      # apply these gradients\n",
    "      self.opt.apply_gradients(zip(new_grads, self.model.trainable_variables),\n",
    "                               global_step=tf.train.get_or_create_global_step())\n",
    "      # current values become new previous values\n",
    "      previous_grads = grads\n",
    "      previous_value = value\n",
    "\n",
    "    return self.env.get_reward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BackwardAgent tends to reach >90% after 20-30k episodes.\n",
    "![backward_acc](BackwardAgent_agent_100.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent(BackwardAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "play_agent(BackwardAgent) # filename='BackwardAgent_agent_100.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Max Pruning a Future Move Tree\n",
    "\n",
    "![leaf_agent](https://adamklecz.files.wordpress.com/2017/10/null6.png)\n",
    "\n",
    "In simple games it is computationally reasonable to survey the possible outcomes in some number of future moves based on the current board state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# may need to run `pip install anytree` to run this\n",
    "from anytree import Node\n",
    "class LeafAgent(AgentBase):\n",
    "  def __init__(self,model,env,lamda = 0.7):\n",
    "    super(LeafAgent,self).__init__(model, env)\n",
    "    self.opt = tf.train.GradientDescentOptimizer(self.lrvar)\n",
    "    self.lamda = lamda\n",
    "\n",
    "  def grad(self,inputs):\n",
    "    # start gradient tape\n",
    "    with tf.GradientTape() as tape:\n",
    "      # calculate reward for inputs using model\n",
    "      reward = self.model(inputs)\n",
    "    # return the reward and the gradients based on this reward\n",
    "    return reward,tape.gradient(reward, self.model.trainable_variables)\n",
    "\n",
    "  def train(self, epsilon):\n",
    "    # reset environment\n",
    "    self.env.reset()\n",
    "\n",
    "    # Our eligibility trace, filled with the current weights of our model\n",
    "    traces = [np.zeros(tvar.shape) for tvar in self.model.trainable_variables]\n",
    "\n",
    "    # current board state (empty)\n",
    "    feature_vector = self.env.make_feature_vector(self.env.board)\n",
    "    # current reward for board state and model gradients\n",
    "    previous_leaf_value, previous_grads = self.grad(feature_vector)\n",
    "    # get first reward from the environment (None)\n",
    "    reward = self.env.get_reward()\n",
    "\n",
    "    # run episode until a player wins\n",
    "    while reward is None:\n",
    "      # get a move, also get the reward and tree describing the move\n",
    "      move, leaf_value, leaf_node = self.get_move(return_value=True)\n",
    "      \n",
    "      # make the move, make random moves randomly\n",
    "      if np.random.rand() < epsilon:\n",
    "        self.env.make_random_move()\n",
    "      else:\n",
    "        self.env.make_move(move)\n",
    "      \n",
    "      # get the updated reward\n",
    "      reward = self.env.get_reward()\n",
    "      # get new feature vector\n",
    "      feature_vector = self.env.make_feature_vector(leaf_node.board)\n",
    "      # calculate grads\n",
    "      _,grads = self.grad(feature_vector)\n",
    "      # update traces\n",
    "      delta = leaf_value - previous_leaf_value\n",
    "      for previous_grad, trace in zip(previous_grads, traces):\n",
    "        trace *= self.lamda\n",
    "        trace += previous_grad.numpy()\n",
    "\n",
    "      # update gradients for this step by the sum of the delta\n",
    "      new_grads = [np.reshape(-delta * t,t.shape) for t in traces]\n",
    "      # apply these gradients\n",
    "      self.opt.apply_gradients(zip(new_grads, self.model.trainable_variables),\n",
    "                               global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "      previous_grads = grads\n",
    "      previous_leaf_value = leaf_value\n",
    "\n",
    "    return self.env.get_reward()\n",
    "\n",
    "  def minimax(self, node, depth, alpha, beta):\n",
    "    # if the current node has a result, return it\n",
    "    if node.board.result() is not None:\n",
    "      value = node.board.result()\n",
    "      return np.array([[value]]), node\n",
    "    # if reached the max depth cutoff, calculate result and return\n",
    "    elif depth <= 0:\n",
    "      fv = self.env.make_feature_vector(node.board)\n",
    "      value = self.model(fv)\n",
    "      return value, node\n",
    "    \n",
    "    # build nodes based on all the legal moves\n",
    "    children = []\n",
    "    # loop over current legal moves\n",
    "    for move in node.board.legal_moves:\n",
    "      # copy board\n",
    "      child_board = node.board.copy()\n",
    "      # make the move (board updated, turn changes)\n",
    "      child_board.push(move)\n",
    "      # create a childe node for this new board, indexed by the move that created the board\n",
    "      child = Node(str(move), parent=node, board=child_board, move=move)\n",
    "      # add to list of child nodes\n",
    "      children.append(child)\n",
    "\n",
    "    # take a turn\n",
    "    if node.board.turn: # X-turn\n",
    "      best_v = -1 # set to lowest value (a loss for X)\n",
    "      best_n = None\n",
    "      # loop over child nodes\n",
    "      for child in children:\n",
    "        # run recursively on this child\n",
    "        value, node = self.minimax(child, depth - 1, alpha, beta)\n",
    "        # is this the highest valued move so far\n",
    "        if value >= best_v:\n",
    "          best_v = value\n",
    "          best_n = node\n",
    "        # alpha starts as -1, and winning == 1 for X\n",
    "        alpha = max(alpha, value)\n",
    "        # can stop if X is winning on this node\n",
    "        if beta <= alpha:\n",
    "          break\n",
    "    else: # O-turn\n",
    "      best_v = 1 # set to the highest value (a loss for O)\n",
    "      best_n = None\n",
    "      # loop over child nodes\n",
    "      for child in children:\n",
    "        # run recursively on this child\n",
    "        value, node = self.minimax(child, depth - 1, alpha, beta)\n",
    "        # is this the lowest valued move so far\n",
    "        if value <= best_v:\n",
    "          best_v = value\n",
    "          best_n = node\n",
    "        # beta starts as 1, and winning == -1 for Y\n",
    "        beta = min(beta, value)\n",
    "        # can stop if Y is winning\n",
    "        if beta <= alpha:\n",
    "          break\n",
    "\n",
    "    return best_v, best_n\n",
    "\n",
    "  def get_move(self, depth=3, return_value=False):\n",
    "    # create a starting node with the current board\n",
    "    node = Node('root', board=self.env.board, move=None)\n",
    "    # start the recursive call to minimax\n",
    "    leaf_value, leaf_node = self.minimax(node, depth, -1, 1)\n",
    "    # return the highest scoring move\n",
    "    if len(leaf_node.path) > 1:\n",
    "      move = leaf_node.path[1].move\n",
    "    # otherwise return the single move\n",
    "\n",
    "    if return_value:\n",
    "      return move, leaf_value, leaf_node\n",
    "    else:\n",
    "      return move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LeafAgent is our most efficient agent when it comes to learning, however, it is the most compute intensive. This means while it may learn in fewer episodes than the previous agents, it still takes a commensurate time to train.\n",
    "\n",
    "![leaf_acc](LeafAgent_agent_100.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_agent(LeafAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_agent(LeafAgent) # filename='LeafAgent_agent_100.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AmiBaRYZ-M7_"
   },
   "source": [
    "# More Fun Examples for RL\n",
    "\n",
    "Using Arcade Learning Environment\n",
    "https://github.com/mgbellemare/Arcade-Learning-Environment\n",
    "\n",
    "Open AI Gym\n",
    "https://gym.openai.com/\n",
    "\n",
    "Google DeepMind's AlphaStar Agent: https://www.youtube.com/watch?v=UuhECwm31dM\n",
    "Google DeepMind's AlphaZero Agents: https://www.youtube.com/watch?v=3N9phq_yZP0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TicTacToeRL.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
