engine:
    # This engine uses the HighThroughputExecutor
    type: GlobusComputeEngine

    available_accelerators: 4 # Assign one worker per GPU
    max_workers_per_node: 4
    
    cpu_affinity: "list:24-31,56-63:16-23,48-55:8-15,40-47:0-7,32-39"
    
    prefetch_capacity: 0  # Increase if you have many more tasks than workers                                              
    max_retries_on_system_failure: 2

    strategy: simple
    job_status_kwargs:
        max_idletime: 300
        strategy_period: 60

    provider:
        type: PBSProProvider

        launcher:
            type: MpiExecLauncher
            # Ensures 1 manger per node, work on all 64 cores
            bind_cmd: --cpu-bind
            overrides: --ppn 1 --env TMPDIR=/tmp

        account: alcf_training
        queue: alcf_training
        cpus_per_node: 64
        select_options: ngpus=4

        # e.g., "#PBS -l filesystems=home:grand:eagle\n#PBS -k doe"
        scheduler_options: "#PBS -l filesystems=home:eagle:grand"

        # Node setup: activate necessary conda environment and such
        worker_init: "module unload xalt; source /grand/alcf_training/workflows/_env/bin/activate; cd $HOME/.globus_compute/workshop-backup"

        walltime: 00:30:00
        nodes_per_block: 1
        init_blocks: 0
        min_blocks: 0
        max_blocks: 1